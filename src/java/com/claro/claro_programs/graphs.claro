# TODO(steving) 1. Support Graph Functions in the Interpreted backend.
# TODO(steving) 2. Try modelling the builtin `input()` function as an async function to queue up interactions with the
# TODO(steving)    console so that graph functions can naturally make use of user input without different threads
# TODO(steving)    stepping all over each other. It also seems more naturally correct for input() to be explicitly
# TODO(steving)    modelled this way anyways since you are consciously waiting on the user for something.
# TODO(steving) 3. If you're able to model builtin `input()` as an async function, then consider extending this logic
# TODO(steving)    a generalized notion of a "SynchronizingResource" so that writes to a shared resource e.g. stdout via
# TODO(steving)    the builtin `print()` and any other following operations are guaranteed to happen as a logical
# TODO(steving)    synchronized unit. This would basically involve keeping a queue of futures to chain off of one
# TODO(steving)    another, where each new call into the SynchronizingResource is applied as a new future chained onto
# TODO(steving)    the last future. Then subsequent calls into the SynchronizingResource would be chained onto the new
# TODO(steving)    last future. All operations must either be a synchronized read, or a synchronized read and write, so
# TODO(steving)    operations passed to mutate should be a lambda of type function<UnwrappedType, UnwrappedType>. Going
# TODO(steving)    based on Java names for this thing, SynchronizedResource == Atomic* except in Claro this is non-blocking.

####################################################################################################
# DEMONSTRATE BASIC GRAPH FUNCTION SEMANTICS
####################################################################################################

# Compute ((3x+1)(x**2 - y))/(x**2) concurrently....which is super ridiculous but demonstrates a point:
#
#
#   times3:(3 * x)                        squared:(x**2)
#       \                                      /      \
#        v                                    v       |
#     plus1:(1 + @times3)    minusY:(@squared - y)    /
#           \                         /              /
#            v                       v              /
#           multiply:(@plus1 * @minusY)            /
#                            \                    /
#                             v                  v
#                         divide:(@multiply / @squared)
#                                     |
#                                     v
#                                   result
graph function firstGraph(x: float, y: float) -> future<float> {
  root result <- @multiply / @squared;

  node multiply <- @plus1 * @minusY;

  node plus1 <- 1 + @times3;

  node minusY <- @squared - y;

  node times3 <- 3 * x;

  node squared <- x * x;
}

# The token <-| is referred to as the "(blocking) unwrap operator". I'm very intentionally not allowing more concise
# ways to unwrap a future (for example w/o a declaration to a variable) because I want blocking to be extremely
# apparent and I don't ever want it to be overlooked as blocking threads can frequently be a bad idea.
var firstConcurrentClaroResult <-| firstGraph(10.0, 2.0);
print(firstConcurrentClaroResult);

####################################################################################################
# DEMONSTRATE BASIC GRAPH PROVIDER SEMANTICS
####################################################################################################
graph provider firstGraphProvider() -> future<float> {
  root providedRes <- @deferredFirstGraphRes;
  node deferredFirstGraphRes <- firstGraph(10.0, 2.0);
}

var firstGraphProviderResult <-| firstGraphProvider();
print(firstGraphProviderResult);

####################################################################################################
# DEMONSTRATE BASIC GRAPH CONSUMER SEMANTICS - (FIRE-AND-FORGET)
####################################################################################################
#
# Note, graph consumer functions should really only be used rarely because you cannot guarantee
# ordering semantics relative to other async code because you are unable to retrieve a handle to the
# work in the form of a future<...> to schedule around. If Claro were to provide a handle, the type
# would have to be something like future<nothing>. But Claro is opposed to meaningless types like
# this so it would end up leading to the addition of a lot of complexity to the language to prevent
# binding any variables to a value of type `nothing` (since by definition that value would not exist).
#
# The killer use case for these existing is for the sake of a server running its infinite loop. The
# server should defer to `graph function` endpoint handlers to get the result, but then needs to be
# able to schedule work after the response is computed to serialize that response to the corresponding
# port and move on to scheduling handling of the next request.
#
# E.g. something like:
#
# consumer startServer(openPorts: [int]) {
#   while (true) {
#     var portsWithRequests: [Request] = getRequestsFromEpoll(openPorts);
#     # Map over a fire-and-forget async operation so the request thread never blocks.
#     map(portsWithRequests, handleRequest(port));
#   }
# }
#
# using(endpointHandler: function<Request -> Response>)
# graph consumer handleRequest(port: responsePort) {
#   root sendResponse <- serializeResponseToPort(port, @response);
#   node response <- endpointHandler(@request);
#   node request <- getRequestFromPort(port);
# }
#
# consumer serializeResponseToPort(port: int, response: Response) {
#   ... # This consumer function is simply going to do some system level IO, but it will have nothing to return.
# }
####################################################################################################

graph consumer graphReturningNothing(message: string) {
  root fireAndForget <- doIO(@formatted);
  node formatted <- "Formatted message: {message}";
}

consumer doIO(s: string) {
  print(s);
}

# Fire-and-forget the graph consumer procedure, it will finish at some point in the future.
var messages = ["execute first consumer graph!", "consumer graph again...", "one last time..."];
for (message in messages) {
  # There's no ordering guarantee for completion of all of the futures that I'm firing-and-forgetting here
  # since each call is potentially running concurrently in different threads. You'll likely need to run this
  # program many times to have a chance of observing that.
  graphReturningNothing(message);
}

####################################################################################################
# DEMONSTRATE LAZY NODE PROVIDER INJECTION
####################################################################################################

graph function demoLazyNodeProviderInjection(x: int, y: int, shouldCallSubgraph: boolean) -> future<int> {
  root res <- @maybeX + @maybeY;
  node maybeX <- maybeCallSubgraph(@xSquared, 0, shouldCallSubgraph);
  node maybeY <- maybeCallSubgraph(@ySquared, 0, shouldCallSubgraph);
  node xSquared <- x * x;
  node ySquared <- y * y;
}

function maybeCallSubgraph(subgraph: provider<future<int>>, other: int, b: boolean) -> future<int> {
  if (b) {
    return subgraph();
  }
  return getIntFuture(other);
}

var callSubgraphRes <-| demoLazyNodeProviderInjection(2, 2, true);
print(callSubgraphRes);
print(callSubgraphRes == 8);
var doNotCallSubgraphRes <-| demoLazyNodeProviderInjection(2, 2, false);
print(doNotCallSubgraphRes);
print(doNotCallSubgraphRes == 0);

####################################################################################################
# DEMONSTRATE RECURSION OVER GRAPH FUNCTIONS & NON-BLOCKING VALIDATION
####################################################################################################
graph function nTimesRecursively(n: int, x: int, y: int) -> future<int> {
  root result <- 1 + @next1;

  # Claro prevents deadlocking by aggressively preventing blocking calls to be reachable from a graph function scope.
  # Try uncommenting one of these lines to see Claro complain at compile-time about the blocking call (even though it's
  # indirect, Claro will still correctly recognize it).
#  node next1 <- maybeRecurseBlockingIndirectly(n - 1 > 0, n, x, y);
#  node next1 <- maybeRecurseBlockingIndirectlyViaFirstClassFunctionDep(maybeRecurseBlockingIndirectly, n - 1 > 0, n, x, y);

  # Claro, however, can correctly determine that this specific call is non-blocking and therefore safe.
#  node next1 <- maybeRecurseBlockingIndirectlyViaFirstClassFunctionDep(nonblocking, n - 1 > 0, n, x, y);

  node next1 <- maybeRecurse(n - 1 > 0, n, x, y);
}

using(immediateFuture: function<int -> future<int>>)
function maybeRecurse(b: boolean, n: int, x: int, y: int) -> future<int> {
  if (b) {
    return nTimesRecursively(n - 1, x, y);
  }
  return immediateFuture(0);
}

blocking function maybeRecurseBlockingIndirectly(b: boolean, n: int, x: int, y: int) -> int {
  return maybeRecurseBlocking(b, n, x, y);
}

blocking function maybeRecurseBlocking(b: boolean, n: int, x: int, y: int) -> int {
  if (b) {
    var res <-| nTimesRecursively(n - 1, x, y);
    return res;
  }
  return 0;
}

blocking:deferFn function maybeRecurseBlockingIndirectlyViaFirstClassFunctionDep(
    deferFn: blocking? function<|boolean, int, int, int| -> int>,
    b: boolean,
    n: int,
    x: int,
    y: int) -> int {
  return deferFn(b, n, x, y);
}

function nonblocking(b: boolean, n: int, x: int, y: int) -> int {
  _ = (n, x, y);
  var res: int = 2;
  if (b) {
    res = 199;
  }
  return res;
}

# TODO(steving) There's got to be an easier way to simply construct an immediate future. Try handling this the way we're
# TODO(steving) already planning to handle implicit auto-wrapping to optional<...> in function returns.
graph function getIntFuture(x: int) -> future<int> {
  root res <- @identity;
  node identity <- x;
}

module ImmediateFutureModule {
  bind immediateFuture:function<int -> future<int>> to getIntFuture;
}

using(ImmediateFutureModule) {
  var n = 200;
  var recursiveRes <-| nTimesRecursively(n, 10, 2);
  print(recursiveRes);
  print(recursiveRes == n);
}

var f: blocking function<|boolean, int, int, int| -> int> = maybeRecurseBlockingIndirectly;
print(f(false, -1, -1, -1));

######################################################################################################
# DEMONSTRATE STATICALLY FORBIDDING ALL MULTITHREADED DATA RACES!
#
# The below examples are saved here for posterity, as these were used in previously demonstrating the
# dangerous patterns that would lead to data races if Claro allowed the use of mutable data w/in graph
# procedures. Each below example fundamentally operated on the principle that you would alias the
# mutable list(s) passed into the graphs, accessing them from separate nodes (that were specially
# arranged to have a high likelihood of ordering switching across reruns) creating an observable data
# race.
#
# The absolute beauty of Claro's Graph Procedures is that Claro is *guaranteed* to statically reject
# all of these possible data races at compile time! It does this by very straightforwardly forbidding
# the passing of mutable data between graph nodes. A graph node expression may do *local* data
# mutation, however, the overall result type of all graph nodes must be *deeply-immutable*. This is a
# foolproof approach to safely sharing node result data across threads. Yes it depends on a limitation
# but you will find power in this limitation. Immutability paired with non-blocking guarantee means
# that by construction you are *guaranteed* to have safe multithreaded code without requiring any sort
# of special expertise in concurrent programming!
#
# Uncomment all of the below if you'd like to see Claro's error messages rejecting the use of mutable
# data that would lead to the data races below. Here are a couple example messages:
#
#   graphs.claro:325: Illegal Mutable Graph Procedure Arg: As Graph Procedures are multi-threaded by nature, all args must be deeply-immutable in order to guarantee that Graph Procedures are data-race free by construction.
#	  	  Found the mutable type:
#	  		  mut [int]
#	  	  To correct this, consider converting the arg's type to its deeply-immutable variant:
#	  	  	[int]
#   graph function unsafeAliasingAllowedOnGivenArgs(l1: mut [int], l2: mut [int]) -> future<boolean> {
#                                                                  ^^
#
#   graphs.claro:327: Illegal Mutable Graph Node Result: As Graph Procedures are multi-threaded by nature, all node expression types must be deeply-immutable in order to guarantee that Graph Procedures are data-race free by construction.
#     		Found the mutable type:
#     			mut [int]
#     		To correct this, consider converting the result of node `update2` to its deeply-immutable variant:
#     			[int]
#       node update2 <- appendAndReturn(l2, 999);
#                       ^^^^^^^^^^^^^^^^^^^^^^^^
#
######################################################################################################
#
#print("TOMFOOLERY ENSUES:");
#
## Even with the exact same arguments to these functions, the output is non-deterministic due to a data race on the list.
## Run them multiple times and you'll see one of 3 possible outcomes for each:
##   Possibility #1: list [1] becomes [1, 2, 999] and prints false
##   Possibility #2: list [1] becomes [1, 999, 2] and prints true
##   Possibility #3: list [1] is appended to literally simultaneously and the list doesn't "realize" that it's been
##                   appended to twice and there's data corruption leaving list [1] as [1, 999] or [1, 2] which in this
##                   case happens to lead to a runtime Panic because the list will be indexed out of bounds. (To observe
##                   this in practice you may need to throw the below graph function call in a while(true) loop. I've
##                   seen it take several hundred thousand iterations to trigger this particular case.)
## (Note: Delay iters were hardcoded to what demonstrated the non-determinism fairly easily on my machine - you might
##        have to think it through and play with different delays on your machine.)
#var unsafeAliasingViaNodeProcedureArgsRes <-| unsafeAliasingViaNodeProcedureArgs(mut [], mut [1]);
#var unsafeALiasingViaDirectNodeAliasingRes <-| unsafeALiasingViaDirectNodeAliasing(mut [1]);
#var unsafeAliasingViaNodeProcedureReturnValueRes <-| unsafeAliasingViaNodeProcedureReturnValue(mut [1]);
#var l = mut [1];
#var unsafeAliasingAllowedOnGivenArgsRes <-| unsafeAliasingAllowedOnGivenArgs(l, l);
#module MutDataModule { bind mutableIntList:mut [int] to mut [1]; }
#var unsafeAliasingViaTransitivelyInjectedMutableValueRes: boolean;
#using(MutDataModule) {
#  var raceRes <-| unsafeAliasingViaTransitivelyInjectedMutableValue();
#  unsafeAliasingViaTransitivelyInjectedMutableValueRes = raceRes;
#}
#
#print(unsafeAliasingViaNodeProcedureArgsRes);
#print(unsafeALiasingViaDirectNodeAliasingRes);
#print(unsafeAliasingViaNodeProcedureReturnValueRes);
#print(unsafeAliasingAllowedOnGivenArgsRes);
#print(unsafeAliasingViaTransitivelyInjectedMutableValueRes);
#
## This is a very complicated example that demonstrates a data race being introduced by unsafe data "aliasing".
#graph function unsafeAliasingViaNodeProcedureArgs(l: mut [mut [int]], l2: mut [int]) -> future<boolean> {
#  # Unsafe "ALIASING" is happening here. Specifically, `l` is appending an "aliased" reference to the same data that
#  # `l2` was already referencing. This would mean that subsequent nodes could be racing to mutate the same underlying
#  # data w/o Claro being able to track it. In the future, Claro will forbid this use of mutable data in Graph nodes.
#  node unsafeAlias <- appendAndReturn(l, l2);
#  node delayedUnsafeAliasLen <- delay(30, len(@unsafeAlias));
#  node update1 <- appendAndReturn((@unsafeAlias)[@delayedUnsafeAliasLen - 1], 2);
#  node update2 <- appendAndReturn(l2, 999);
#  root observeRaceRes <- printAndReturn(@update1, @update2);
#}
#
## The following graph also introduces unsafe Aliasing albeit via DIRECT NODE ALIASING.
#graph function unsafeALiasingViaDirectNodeAliasing(l: mut [int]) -> future<boolean> {
#  node unsafeAlias <- l;
#  node delayedUnsafeAlias <- delay(30, @unsafeAlias);
#  node update1 <- appendAndReturn(@delayedUnsafeAlias, 2);
#  node update2 <- appendAndReturn(l, 999);
#  root observeRaceRes <- printAndReturn(@update1, @update2);
#}
#
## The following graph also introduces unsafe Aliasing albeit via NODE PROCEDURE RETURN VALUE.
#graph function unsafeAliasingViaNodeProcedureReturnValue(l: mut [int]) -> future<boolean> {
#  node unsafeAlias <- delay(30, l);
#  node update1 <- appendAndReturn(@unsafeAlias, 2);
#  node update2 <- appendAndReturn(l, 999);
#  root observeRaceRes <- printAndReturn(@update1, @update2);
#}
#
## Finally, the following graph also introduces unsafe Aliasing albeit via a very sneaky situation where everything in
## the graph procedure itself is actually defined totally correctly, introducing no new aliasing, however if you call it
## with already aliased data, then Claro would be unable to check for data races using a technique local to graph procedures.
#graph function unsafeAliasingAllowedOnGivenArgs(l1: mut [int], l2: mut [int]) -> future<boolean> {
#  node update1 <- appendAndReturnDelayed(l1, 2, 20);
#  node update2 <- appendAndReturn(l2, 999);
#  root observeRaceRes <- printAndReturn(@update1, @update2);
#}
#
## This graph is interesting in the sense that the graph itself is completely valid, but the procedures it depends on
## introduce unsafe aliasing of shared mutable state by injecting the same mutable list.
#graph provider unsafeAliasingViaTransitivelyInjectedMutableValue() -> future<boolean> {
#  node update1 <- appendToAndReturnInjectedMutableIntListDelayed(2, 12);
#  node update2 <- appendToAndReturnInjectedMutableIntList(999);
#  root observeRaceRes <- printAndReturn(copy(@update1), copy(@update2));
#}
#
#using(mutableIntList:mut [int])
#function appendToAndReturnInjectedMutableIntList(x: int) -> [int] {
#  return copy(appendAndReturn(mutableIntList, x));
#}
#
#function appendToAndReturnInjectedMutableIntListDelayed(x: int, iters: int) -> [int] {
#  _ = delay(iters, "");
#  return copy(appendToAndReturnInjectedMutableIntList(x));
#}
#
## TODO: Add builtin support for a sleep timer to make better examples.
#function delay<T>(iters: int, res: T) -> T {
#  var i = 0;
#  var s = "";
#  while (i++ < iters) {
#    print(i);
#    s = "{s} ";
#  }
#  return res;
#}
#
#function printAndReturn(l1: mut [int], l2: mut [int]) -> boolean {
#  print(l1);
#  print(l2);
#  return l1[1] > l2[2];
#}
#
#function appendAndReturn<T>(l: mut [T], toAppend: T) -> mut [T] {
#  append(l, toAppend);
#  return l;
#}
#
#function appendAndReturnDelayed<T>(l: mut [T], toAppend: T, iters: int) -> mut [T] {
#  _ = delay(iters, "");
#  return appendAndReturn(l, toAppend);
#}